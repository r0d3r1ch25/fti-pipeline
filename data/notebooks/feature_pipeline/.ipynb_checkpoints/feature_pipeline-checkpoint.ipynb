{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8bc30ec-26ca-4130-9a1d-e09c813a320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2799c5-4370-473c-95d0-e538fc9e0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "import pickle\n",
    "import json\n",
    "import io\n",
    "\n",
    "\n",
    "class InMemoryBuffer:\n",
    "    \"\"\"Buffer to store recent observations for rolling feature computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.observation_count = 0  # Track total observations seen\n",
    "    \n",
    "    def add(self, observations: Union[pd.Series, pd.DataFrame]) -> None:\n",
    "        \"\"\"Add new observation(s) to buffer.\"\"\"\n",
    "        if isinstance(observations, pd.Series):\n",
    "            self.buffer.append(observations)\n",
    "            self.observation_count += 1\n",
    "        elif isinstance(observations, pd.DataFrame):\n",
    "            for _, row in observations.iterrows():\n",
    "                self.buffer.append(row)\n",
    "                self.observation_count += 1\n",
    "    \n",
    "    def get_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Return buffer contents as DataFrame.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(list(self.buffer))\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get buffer state for persistence.\"\"\"\n",
    "        return {\n",
    "            'max_size': self.max_size,\n",
    "            'buffer_data': [obs.to_dict() if hasattr(obs, 'to_dict') else obs for obs in self.buffer],\n",
    "            'observation_count': self.observation_count\n",
    "        }\n",
    "    \n",
    "    def load_state(self, state: Dict[str, Any]) -> None:\n",
    "        \"\"\"Load buffer state from persistence.\"\"\"\n",
    "        self.max_size = state['max_size']\n",
    "        self.observation_count = state['observation_count']\n",
    "        self.buffer = deque(maxlen=self.max_size)\n",
    "        \n",
    "        for obs_dict in state['buffer_data']:\n",
    "            if isinstance(obs_dict, dict):\n",
    "                obs = pd.Series(obs_dict)\n",
    "                self.buffer.append(obs)\n",
    "\n",
    "\n",
    "class ScalerManager:\n",
    "    \"\"\"Manages scaler state and incremental updates with persistence support.\"\"\"\n",
    "    \n",
    "    def __init__(self, scaler, update_enabled: bool = True, update_freq: int = 1):\n",
    "        self.base_scaler = scaler\n",
    "        self.update_enabled = update_enabled\n",
    "        self.update_freq = update_freq\n",
    "        self.update_counter = 0\n",
    "        self.scaler_version = 0\n",
    "        self.last_update_count = 0\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame) -> None:\n",
    "        \"\"\"Initial fit of the scaler.\"\"\"\n",
    "        self.base_scaler.fit(X)\n",
    "        self.scaler_version += 1\n",
    "        \n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Transform data using current scaler state.\"\"\"\n",
    "        return self.base_scaler.transform(X)\n",
    "        \n",
    "    def partial_update(self, X: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Incrementally update scaler if conditions are met.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if scaler was updated, False otherwise\n",
    "        \"\"\"\n",
    "        if not self.update_enabled:\n",
    "            return False\n",
    "            \n",
    "        self.update_counter += len(X)\n",
    "        \n",
    "        # Check if we should update based on frequency\n",
    "        if self.update_counter - self.last_update_count >= self.update_freq:\n",
    "            if hasattr(self.base_scaler, 'partial_fit'):\n",
    "                self.base_scaler.partial_fit(X)\n",
    "                self.scaler_version += 1\n",
    "                self.last_update_count = self.update_counter\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Warning: Scaler doesn't support partial_fit\")\n",
    "                return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get scaler state for persistence.\"\"\"\n",
    "        return {\n",
    "            'scaler_state': pickle.dumps(self.base_scaler),\n",
    "            'update_enabled': self.update_enabled,\n",
    "            'update_freq': self.update_freq,\n",
    "            'update_counter': self.update_counter,\n",
    "            'scaler_version': self.scaler_version,\n",
    "            'last_update_count': self.last_update_count\n",
    "        }\n",
    "    \n",
    "    def load_state(self, state: Dict[str, Any]) -> None:\n",
    "        \"\"\"Load scaler state from persistence.\"\"\"\n",
    "        self.base_scaler = pickle.loads(state['scaler_state'])\n",
    "        self.update_enabled = state['update_enabled']\n",
    "        self.update_freq = state['update_freq']\n",
    "        self.update_counter = state['update_counter']\n",
    "        self.scaler_version = state['scaler_version']\n",
    "        self.last_update_count = state['last_update_count']\n",
    "\n",
    "\n",
    "class InMemoryBuffer:\n",
    "    \"\"\"Buffer to store recent observations for rolling feature computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.observation_count = 0  # Track total observations seen\n",
    "    \n",
    "    def add(self, observations: Union[pd.Series, pd.DataFrame]) -> None:\n",
    "        \"\"\"Add new observation(s) to buffer.\"\"\"\n",
    "        if isinstance(observations, pd.Series):\n",
    "            self.buffer.append(observations)\n",
    "            self.observation_count += 1\n",
    "        elif isinstance(observations, pd.DataFrame):\n",
    "            for _, row in observations.iterrows():\n",
    "                self.buffer.append(row)\n",
    "                self.observation_count += 1\n",
    "    \n",
    "    def get_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Return buffer contents as DataFrame.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(list(self.buffer))\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get buffer state for persistence.\"\"\"\n",
    "        buffer_data = []\n",
    "        for obs in self.buffer:\n",
    "            if hasattr(obs, 'to_dict'):\n",
    "                # Handle pandas Series\n",
    "                obs_dict = obs.to_dict()\n",
    "                # Also store the index if it exists\n",
    "                if hasattr(obs, 'name') and obs.name is not None:\n",
    "                    obs_dict['_index'] = str(obs.name)\n",
    "                buffer_data.append(obs_dict)\n",
    "            else:\n",
    "                buffer_data.append(obs)\n",
    "        \n",
    "        return {\n",
    "            'max_size': self.max_size,\n",
    "            'buffer_data': buffer_data,\n",
    "            'observation_count': self.observation_count\n",
    "        }\n",
    "    \n",
    "    def load_state(self, state: Dict[str, Any]) -> None:\n",
    "        \"\"\"Load buffer state from persistence.\"\"\"\n",
    "        self.max_size = state['max_size']\n",
    "        self.observation_count = state['observation_count']\n",
    "        self.buffer = deque(maxlen=self.max_size)\n",
    "        \n",
    "        for obs_dict in state['buffer_data']:\n",
    "            if isinstance(obs_dict, dict) and '_index' in obs_dict:\n",
    "                # Reconstruct pandas Series with index\n",
    "                index_val = obs_dict.pop('_index')\n",
    "                obs = pd.Series(obs_dict, name=index_val)\n",
    "                self.buffer.append(obs)\n",
    "            elif isinstance(obs_dict, dict):\n",
    "                obs = pd.Series(obs_dict)\n",
    "                self.buffer.append(obs)\n",
    "            else:\n",
    "                self.buffer.append(obs_dict)\n",
    "\n",
    "\n",
    "class TimeSeriesVW:\n",
    "    \"\"\"\n",
    "    Time Series feature engineering class with Vowpal Wabbit output format.\n",
    "    Supports incremental learning and online inference with evolving scaler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler_manager, buffer, max_window: int = 6, horizon: int = 6, \n",
    "                 target_col: str = 'target'):\n",
    "        \"\"\"\n",
    "        Initialize TimeSeriesVW with dependency injection.\n",
    "        \n",
    "        Args:\n",
    "            scaler_manager: ScalerManager instance for handling scaler evolution\n",
    "            buffer: Buffer instance for storing recent observations\n",
    "            max_window: Maximum window size for rolling features\n",
    "            horizon: Forecasting horizon\n",
    "            target_col: Name of target column\n",
    "        \"\"\"\n",
    "        self.scaler_manager = scaler_manager\n",
    "        self.buffer = buffer\n",
    "        self.max_window = max_window\n",
    "        self.horizon = horizon\n",
    "        self.target_col = target_col\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _create_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create rolling features for given DataFrame.\"\"\"\n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # Create rolling features for different lag windows\n",
    "        for lag in range(1, self.max_window + 1):\n",
    "            # Rolling mean\n",
    "            df_features[f'rolling_mean_{lag}'] = df[self.target_col].rolling(window=lag, min_periods=1).mean()\n",
    "            \n",
    "            # Rolling std\n",
    "            df_features[f'rolling_std_{lag}'] = df[self.target_col].rolling(window=lag, min_periods=1).std().fillna(0)\n",
    "            \n",
    "            # Percentage change\n",
    "            df_features[f'pct_{lag}'] = df[self.target_col].pct_change(periods=lag).fillna(0)\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create temporal features (month and quarter one-hot encoding).\"\"\"\n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # Extract month and quarter from datetime index\n",
    "        df_features['month_num'] = df_features.index.month\n",
    "        df_features['quarter'] = df_features.index.quarter\n",
    "        \n",
    "        # One-hot encoding for month (1-12)\n",
    "        for month in range(1, 13):\n",
    "            df_features[f'month_{month}'] = (df_features['month_num'] == month).astype(int)\n",
    "        \n",
    "        # One-hot encoding for quarter (1-4)\n",
    "        for quarter in range(1, 5):\n",
    "            df_features[f'quarter_{quarter}'] = (df_features['quarter'] == quarter).astype(int)\n",
    "        \n",
    "        # Drop intermediate columns\n",
    "        df_features = df_features.drop(['month_num', 'quarter'], axis=1)\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def _create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create all features for the dataset.\"\"\"\n",
    "        # Create rolling features\n",
    "        df_features = self._create_rolling_features(df)\n",
    "        \n",
    "        # Create temporal features\n",
    "        df_features = self._create_temporal_features(df_features)\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Get feature column names (excluding target).\"\"\"\n",
    "        return [col for col in df.columns if col != self.target_col]\n",
    "    \n",
    "    def fit_features(self, df: pd.DataFrame) -> 'TimeSeriesVW':\n",
    "        \"\"\"\n",
    "        Fit the scaler on training data only.\n",
    "        \n",
    "        Args:\n",
    "            df: Training DataFrame with datetime index\n",
    "            \n",
    "        Returns:\n",
    "            self for method chaining\n",
    "        \"\"\"\n",
    "        # Create features\n",
    "        df_features = self._create_features(df)\n",
    "        \n",
    "        # Get feature columns\n",
    "        feature_cols = self._get_feature_columns(df_features)\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        # Fit scaler on training features only\n",
    "        self.scaler_manager.fit(df_features[feature_cols])\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit scaler and transform features for the full dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: Full DataFrame with datetime index\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with engineered and scaled features\n",
    "        \"\"\"\n",
    "        # Fit on the data\n",
    "        self.fit_features(df)\n",
    "        \n",
    "        # Transform features\n",
    "        transformed_df = self.transform_features(df)\n",
    "        \n",
    "        # CRITICAL: Update buffer with the last max_window-1 observations\n",
    "        # This ensures rolling features work correctly for future incremental data\n",
    "        last_obs = df.tail(self.max_window - 1)\n",
    "        self.buffer.add(last_obs)\n",
    "        \n",
    "        return transformed_df\n",
    "    \n",
    "    def transform_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform features using fitted scaler and buffer for rolling features.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to transform (can be single row or batch)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with transformed features\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must call fit_features() before transform_features()\")\n",
    "        \n",
    "        # For incremental processing, combine buffer with new data for rolling features\n",
    "        if self.buffer.size() > 0:\n",
    "            buffer_df = self.buffer.get_data()\n",
    "            # Combine buffer with new data for rolling calculations\n",
    "            combined_df = pd.concat([buffer_df, df], ignore_index=False)\n",
    "            # Take only the new data rows after computing rolling features\n",
    "            df_features = self._create_features(combined_df)\n",
    "            df_features = df_features.iloc[-len(df):]\n",
    "        else:\n",
    "            df_features = self._create_features(df)\n",
    "        \n",
    "        # Create temporal features (these don't depend on buffer)\n",
    "        df_features = self._create_temporal_features(df_features)\n",
    "        \n",
    "        # Scale features\n",
    "        feature_cols = self.feature_names\n",
    "        df_features[feature_cols] = self.scaler_manager.transform(df_features[feature_cols])\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def _update_scaler_incremental(self, new_features: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Update scaler incrementally with new data using partial_fit if available.\n",
    "        Falls back to full refit if partial_fit not supported.\n",
    "        \"\"\"\n",
    "        feature_cols = self.feature_names\n",
    "        new_feature_data = new_features[feature_cols]\n",
    "        \n",
    "        # Use ScalerManager to handle incremental updates\n",
    "        updated = self.scaler_manager.partial_update(new_feature_data)\n",
    "        if updated:\n",
    "            print(f\"Scaler updated to version {self.scaler_manager.scaler_version}\")\n",
    "    \n",
    "    def update(self, new_obs: Union[pd.Series, pd.DataFrame]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Update buffer with new observation(s), update scaler, and generate VW format lines.\n",
    "        \n",
    "        Args:\n",
    "            new_obs: New observation(s) to add to buffer\n",
    "            \n",
    "        Returns:\n",
    "            List of VW format strings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must call fit_features() before update()\")\n",
    "        \n",
    "        # Convert Series to DataFrame if needed\n",
    "        if isinstance(new_obs, pd.Series):\n",
    "            new_obs = new_obs.to_frame().T\n",
    "        \n",
    "        # Transform features (using current scaler state)\n",
    "        transformed_obs = self.transform_features(new_obs)\n",
    "        \n",
    "        # Update scaler with new transformed features\n",
    "        self._update_scaler_incremental(transformed_obs)\n",
    "        \n",
    "        # Update buffer with original observations (for future rolling calculations)\n",
    "        self.buffer.add(new_obs)\n",
    "        \n",
    "        # Generate VW lines\n",
    "        vw_lines = []\n",
    "        for idx, row in transformed_obs.iterrows():\n",
    "            # Create ID from index (timestamp)\n",
    "            obs_id = str(idx) if hasattr(idx, 'strftime') else str(idx)\n",
    "            vw_line = self._create_vw_line(row, obs_id)\n",
    "            vw_lines.append(vw_line)\n",
    "        \n",
    "        return vw_lines\n",
    "    \n",
    "    def save_state(self, path: str) -> None:\n",
    "        \"\"\"Save complete model state for persistence (K8s readiness).\"\"\"\n",
    "        state = {\n",
    "            'scaler_manager_state': self.scaler_manager.get_state(),\n",
    "            'buffer_state': self.buffer.get_state(),\n",
    "            'max_window': self.max_window,\n",
    "            'horizon': self.horizon,\n",
    "            'target_col': self.target_col,\n",
    "            'feature_names': self.feature_names,\n",
    "            'is_fitted': self.is_fitted\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        \n",
    "        print(f\"Model state saved to {path}\")\n",
    "    \n",
    "    def load_state(self, path: str) -> None:\n",
    "        \"\"\"Load complete model state from persistence.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "        \n",
    "        self.scaler_manager.load_state(state['scaler_manager_state'])\n",
    "        self.buffer.load_state(state['buffer_state'])\n",
    "        self.max_window = state['max_window']\n",
    "        self.horizon = state['horizon']\n",
    "        self.target_col = state['target_col']\n",
    "        self.feature_names = state['feature_names']\n",
    "        self.is_fitted = state['is_fitted']\n",
    "        \n",
    "        print(f\"Model state loaded from {path}\")\n",
    "        print(f\"Scaler version: {self.scaler_manager.scaler_version}\")\n",
    "        print(f\"Buffer size: {self.buffer.size()}\")\n",
    "    \n",
    "    def _create_vw_line(self, row: pd.Series, obs_id: str) -> str:\n",
    "        \"\"\"Create a single VW format line from a row.\"\"\"\n",
    "        target_value = row[self.target_col]\n",
    "        feature_cols = [col for col in row.index if col != self.target_col]\n",
    "        \n",
    "        # Build feature string\n",
    "        features = []\n",
    "        for col in feature_cols:\n",
    "            value = row[col]\n",
    "            # Handle NaN values\n",
    "            if pd.isna(value):\n",
    "                value = 0.0\n",
    "            features.append(f\"{col}:{value:.6f}\")\n",
    "        \n",
    "        feature_str = \" \".join(features)\n",
    "        vw_line = f\"{target_value:.6f} |features {feature_str} ID:{obs_id}\"\n",
    "        \n",
    "        return vw_line\n",
    "    \n",
    "    def to_vw(self, df: pd.DataFrame, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save DataFrame to Vowpal Wabbit format.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with features and target\n",
    "            path: Output file path\n",
    "        \"\"\"\n",
    "        vw_lines = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            obs_id = str(idx) if hasattr(idx, 'strftime') else str(idx)\n",
    "            vw_line = self._create_vw_line(row, obs_id)\n",
    "            vw_lines.append(vw_line)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(path, 'w') as f:\n",
    "            f.write('\\n'.join(vw_lines))\n",
    "        \n",
    "        print(f\"Saved {len(vw_lines)} observations to {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528dc85f-9d62-4cba-b8c3-7e3445ff9bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (144, 2)\n",
      "Dataset head:\n",
      "            passengers  target\n",
      "month                         \n",
      "1949-01-01         112     112\n",
      "1949-02-01         118     118\n",
      "1949-03-01         132     132\n",
      "1949-04-01         129     129\n",
      "1949-05-01         121     121\n",
      "\n",
      "Data splits:\n",
      "Train: 132 observations\n",
      "Validation: 6 observations\n",
      "Test: 6 observations\n",
      "\n",
      "Train features shape: (132, 36)\n",
      "Feature columns: ['passengers', 'rolling_mean_1', 'rolling_std_1', 'pct_1', 'rolling_mean_2', 'rolling_std_2', 'pct_2', 'rolling_mean_3', 'rolling_std_3', 'pct_3', 'rolling_mean_4', 'rolling_std_4', 'pct_4', 'rolling_mean_5', 'rolling_std_5', 'pct_5', 'rolling_mean_6', 'rolling_std_6', 'pct_6', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'quarter_1', 'quarter_2', 'quarter_3', 'quarter_4']\n",
      "Validation features shape: (6, 36)\n",
      "Test features shape: (6, 36)\n",
      "Buffer size after validation transform: 5\n",
      "Saved 132 observations to train.vw\n",
      "Saved 6 observations to validation.vw\n",
      "Saved 6 observations to test.vw\n",
      "\n",
      "=== Incremental Learning Demo ===\n",
      "\n",
      "=== Incremental Learning Demo ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'InMemoryBuffer' object has no attribute 'get_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Incremental Learning Demo ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Save state after training (K8s checkpoint)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mts_vw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_checkpoint.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Simulate loading state (like K8s pod restart)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m scaler_fresh \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "Cell \u001b[0;32mIn[2], line 361\u001b[0m, in \u001b[0;36mTimeSeriesVW.save_state\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save complete model state for persistence (K8s readiness).\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler_manager_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_manager\u001b[38;5;241m.\u001b[39mget_state(),\n\u001b[0;32m--> 361\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m(),\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_window\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_window,\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon,\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_col\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_col,\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names,\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_fitted\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted\n\u001b[1;32m    367\u001b[0m     }\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    370\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(state, f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InMemoryBuffer' object has no attribute 'get_state'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv('air.csv')\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    df = df.set_index('month')\n",
    "    df['target'] = df['passengers']  # Create target column\n",
    "    \n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    print(\"Dataset head:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Initialize dependencies with proper scaler management\n",
    "    scaler = StandardScaler()  # Base scaler\n",
    "    scaler_manager = ScalerManager(\n",
    "        scaler=scaler,\n",
    "        update_enabled=True,  # Enable incremental updates\n",
    "        update_freq=1  # Update every observation\n",
    "    )\n",
    "    buffer = InMemoryBuffer(max_size=5)  # max_window - 1\n",
    "    \n",
    "    # Initialize TimeSeriesVW with ScalerManager\n",
    "    ts_vw = TimeSeriesVW(\n",
    "        scaler_manager=scaler_manager,\n",
    "        buffer=buffer,\n",
    "        max_window=6,\n",
    "        horizon=6,\n",
    "        target_col='target'\n",
    "    )\n",
    "    \n",
    "    # Train/validation/test split\n",
    "    total_len = len(df)\n",
    "    train_size = total_len - 12  # Last 12 for validation + test\n",
    "    val_size = 6\n",
    "    test_size = 6\n",
    "    \n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size:train_size + val_size]\n",
    "    test_df = df.iloc[train_size + val_size:]\n",
    "    \n",
    "    print(f\"\\nData splits:\")\n",
    "    print(f\"Train: {len(train_df)} observations\")\n",
    "    print(f\"Validation: {len(val_df)} observations\") \n",
    "    print(f\"Test: {len(test_df)} observations\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    train_transformed = ts_vw.fit_transform_features(train_df)\n",
    "    print(f\"\\nTrain features shape: {train_transformed.shape}\")\n",
    "    print(\"Feature columns:\", [col for col in train_transformed.columns if col != 'target'])\n",
    "    \n",
    "    # Transform validation and test data\n",
    "    val_transformed = ts_vw.transform_features(val_df)\n",
    "    test_transformed = ts_vw.transform_features(test_df)\n",
    "    \n",
    "    print(f\"Validation features shape: {val_transformed.shape}\")\n",
    "    print(f\"Test features shape: {test_transformed.shape}\")\n",
    "    print(f\"Buffer size after validation transform: {ts_vw.buffer.size()}\")  # Should still be 5\n",
    "    \n",
    "    # Save to VW format\n",
    "    ts_vw.to_vw(train_transformed, 'train.vw')\n",
    "    ts_vw.to_vw(val_transformed, 'validation.vw')\n",
    "    ts_vw.to_vw(test_transformed, 'test.vw')\n",
    "    \n",
    "    # Demonstrate incremental learning\n",
    "    print(f\"\\n=== Incremental Learning Demo ===\")\n",
    "    \n",
    "    # Incremental learning demo with proper state management\n",
    "    print(f\"\\n=== Incremental Learning Demo ===\")\n",
    "    \n",
    "    # Save state after training (K8s checkpoint)\n",
    "    ts_vw.save_state('model_checkpoint.pkl')\n",
    "    \n",
    "    # Simulate loading state (like K8s pod restart)\n",
    "    scaler_fresh = StandardScaler()\n",
    "    scaler_manager_fresh = ScalerManager(scaler=scaler_fresh, update_enabled=True, update_freq=1)\n",
    "    buffer_fresh = InMemoryBuffer(max_size=5)\n",
    "    \n",
    "    ts_vw_reloaded = TimeSeriesVW(\n",
    "        scaler_manager=scaler_manager_fresh,\n",
    "        buffer=buffer_fresh,\n",
    "        max_window=6,\n",
    "        horizon=6,\n",
    "        target_col='target'\n",
    "    )\n",
    "    \n",
    "    # Load saved state\n",
    "    ts_vw_reloaded.load_state('model_checkpoint.pkl')\n",
    "    \n",
    "    print(f\"State reloaded - Buffer size: {ts_vw_reloaded.buffer.size()}\")\n",
    "    print(f\"Scaler version: {ts_vw_reloaded.scaler_manager.scaler_version}\")\n",
    "    \n",
    "    # Incrementally process validation data with scaler updates\n",
    "    print(\"\\nIncremental processing with scaler updates:\")\n",
    "    print(\"Scaler mean before updates:\", ts_vw_reloaded.scaler_manager.base_scaler.mean_[:3])\n",
    "    \n",
    "    for i, (idx, row) in enumerate(val_df.iterrows()):\n",
    "        vw_lines = ts_vw_reloaded.update(row)\n",
    "        print(f\"Observation {i+1} ({idx.strftime('%Y-%m')}):\")\n",
    "        print(f\"  Target: {row['target']:.2f}\")\n",
    "        print(f\"  VW line: {vw_lines[0][:80]}...\")\n",
    "        print(f\"  Buffer size: {ts_vw_reloaded.buffer.size()}\")\n",
    "        print(f\"  Scaler version: {ts_vw_reloaded.scaler_manager.scaler_version}\")\n",
    "        if i == 0:  # Show scaler evolution after first update\n",
    "            print(f\"  Scaler mean after update: {ts_vw_reloaded.scaler_manager.base_scaler.mean_[:3]}\")\n",
    "    \n",
    "    print(f\"Final scaler mean: {ts_vw_reloaded.scaler_manager.base_scaler.mean_[:3]}\")\n",
    "    print(f\"Total scaler updates: {ts_vw_reloaded.scaler_manager.update_counter}\")\n",
    "    \n",
    "    # Save final state\n",
    "    ts_vw_reloaded.save_state('model_final_state.pkl')\n",
    "    \n",
    "    # Batch incremental processing\n",
    "    print(f\"\\n=== Batch Incremental Processing ===\")\n",
    "    \n",
    "    # Reset buffer and process test data in batch with different update frequency\n",
    "    buffer_batch = InMemoryBuffer(max_size=5)\n",
    "    ts_vw_batch = TimeSeriesVW(\n",
    "        scaler=StandardScaler(),\n",
    "        buffer=buffer_batch,\n",
    "        max_window=6,\n",
    "        horizon=6,\n",
    "        target_col='target',\n",
    "        update_scaler=True,\n",
    "        scaler_update_freq=3  # Update scaler every 3 observations (batch strategy)\n",
    "    )\n",
    "    ts_vw_batch.fit_features(train_df)\n",
    "    \n",
    "    # For batch demo, we need to populate the buffer first\n",
    "    last_train_obs = train_df.tail(ts_vw_batch.max_window - 1)\n",
    "    ts_vw_batch.buffer.add(last_train_obs)\n",
    "    \n",
    "    # Process validation data as batch\n",
    "    print(f\"Scaler mean before batch update: {ts_vw_batch.scaler.mean_[:3]}\")\n",
    "    vw_lines_batch = ts_vw_batch.update(val_df)\n",
    "    print(f\"Processed {len(vw_lines_batch)} observations in batch\")\n",
    "    print(f\"Scaler mean after batch update: {ts_vw_batch.scaler.mean_[:3]}\")\n",
    "    print(\"First VW line:\", vw_lines_batch[0][:100] + \"...\")\n",
    "    print(\"Last VW line:\", vw_lines_batch[-1][:100] + \"...\")\n",
    "    \n",
    "    # Show feature engineering example\n",
    "    print(f\"\\n=== Feature Engineering Example ===\")\n",
    "    sample_transformed = train_transformed.head(3)\n",
    "    print(\"Sample transformed data (first 3 rows):\")\n",
    "    feature_cols = [col for col in sample_transformed.columns if col != 'target']\n",
    "    print(f\"Features created: {len(feature_cols)} features\")\n",
    "    \n",
    "    # Show some key features\n",
    "    key_features = [col for col in feature_cols if any(x in col for x in ['rolling_mean', 'month_', 'quarter_'])][:10]\n",
    "    print(f\"Sample key features: {key_features}\")\n",
    "    \n",
    "    for idx, row in sample_transformed.head(2).iterrows():\n",
    "        print(f\"\\nDate: {idx.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Target: {row[ts_vw.target_col]:.2f}\")\n",
    "        for feat in key_features[:5]:\n",
    "            print(f\"  {feat}: {row[feat]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"✓ Created {len(feature_cols)} features including rolling stats and temporal features\")\n",
    "    print(f\"✓ Scaler fitted on {len(train_df)} training observations\")\n",
    "    print(f\"✓ Generated VW files for train/validation/test\")\n",
    "    print(f\"✓ Implemented incremental scaler updates with versioning\")\n",
    "    print(f\"✓ Added state persistence for K8s pod restarts\")\n",
    "    print(f\"✓ Buffer properly maintains rolling window context\")\n",
    "    print(f\"✓ Ready for Redis migration (state serialization included)\")\n",
    "    print(f\"✓ Production-ready for K8s cluster with online learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46880cf4-c5b3-44ae-9f34-84b4eb39a95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
